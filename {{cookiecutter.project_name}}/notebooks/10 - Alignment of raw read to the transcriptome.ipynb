{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment raw reads to the Transcriptome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../config/init.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = os.path.join(RESULTS, DATASET, 'alignments')\n",
    "submission_dir = os.path.join(RESULTS, DATASET, 'submission')\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir) \n",
    "    \n",
    "fasta_dir = os.path.join(result_dir, 'fasta_genbank_ids')\n",
    "if not os.path.exists(fasta_dir):\n",
    "    os.mkdir(fasta_dir) \n",
    "\n",
    "\n",
    "sra_df = pandas.read_csv(os.path.join(DATA, DATASET, 'sample_table.csv'), header=None)\n",
    "sra_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting submission file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_files = [f.replace('.fsa', '') for dr, ds, files in os.walk(submission_dir) for f in files if f.endswith('.fsa')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing FASTA IDS to use the new GenBank IDs\n",
    "\n",
    "From the TSA submission download the accession list to the submission directory.\n",
    "Set the `ACCESSION_FILE` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESSION_FILE = 'GISG03_accs'\n",
    "accs = pandas.read_csv(os.path.join(submission_dir, ACCESSION_FILE), sep='\\t', header=None)\n",
    "accs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a new Fasta file with submitted transcripts\n",
    "\n",
    "A new file named *transcriptome.fsa* will be created with the submitted transcripts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(fasta_dir)\n",
    "bar_length = 100\n",
    "if not os.path.exists('transcriptome.fsa'):\n",
    "    with open('transcriptome.fsa', \"w\") as output_handle:\n",
    "        for s in splitted_files:\n",
    "            sname = os.path.join(submission_dir, s + '.fsa')\n",
    "            if os.path.exists(sname):\n",
    "                total = !grep -c \"^>\" {sname}\n",
    "                total = int(''.join(total))\n",
    "                print('Processing sample: {} with {} transcripts'.format(s,total))\n",
    "                count = 0\n",
    "                for record in SeqIO.parse(sname, 'fasta'): \n",
    "                    record.id = accs[accs[0] == record.id][1].iloc[0]\n",
    "                    record.description = ''\n",
    "                    count += 1\n",
    "                    percent = count * 100/total\n",
    "                    SeqIO.write(record, output_handle, \"fasta\")\n",
    "                    print('{:6d} [{}] {:3.1f}%'.format(count, \n",
    "                                                       \"#\" * int(percent) + \"-\" * (bar_length - int(percent)), \n",
    "                                                       percent), \n",
    "                          end='\\r')\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing gcloud configurationÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZONE = 'us-east4'\n",
    "REGION = 'us-east4-c'\n",
    "!gcloud --version\n",
    "account = !gcloud config get-value account\n",
    "account = ''.join(account)\n",
    "project = !gcloud config get-value project\n",
    "project = ''.join(project)\n",
    "if account != '(unset)' and project != '(unset)':\n",
    "    print('Using account: {} with project: {}'.format(account, project))\n",
    "else:\n",
    "    print('Please, configure Cloud SDK before running this notebook')\n",
    "    print('Open a Terminal and run: gcloud init')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create GCP bucket for SRA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(fasta_dir)\n",
    "bucket_list = !gsutil ls\n",
    "bucket = None\n",
    "\n",
    "prefix = 'gs://{}-vector-'.format(DATASET.lower())\n",
    "for l in bucket_list:\n",
    "    if prefix in l:\n",
    "        bucket = l.replace('gs://{}-vector-'.format(DATASET.lower()),'').replace('/','')\n",
    "        break\n",
    "\n",
    "inbucket = '{}-vector-{}'.format(DATASET.lower(), bucket)\n",
    "outbucket = '{}-align-{}'.format(DATASET.lower(), bucket)\n",
    "\n",
    "bucket_list = !gsutil ls gs://{outbucket}\n",
    "if ''.join(bucket_list).startswith('BucketNotFoundException'):\n",
    "    !gsutil mb gs://{outbucket}\n",
    "    !gsutil -m cp -R transcriptome.fsa gs://{inbucket}/\n",
    "\n",
    "print('input bucket: {}'.format(inbucket))\n",
    "print('output bucket: {}'.format(outbucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_JSON = os.path.join(BIN, 'gcp', 'pipeline-read-assignment.json')\n",
    "\n",
    "operations_alignment = {}\n",
    "\n",
    "op_dir = os.path.join(result_dir, 'gcp')\n",
    "if not os.path.exists(op_dir):\n",
    "    os.mkdir(op_dir) \n",
    "os.chdir(op_dir)\n",
    "                \n",
    "if os.path.exists('operations-alignment.tsv'):\n",
    "    operations_alignment['logs'] = {}\n",
    "    operations_alignment['operations'] = pandas.read_csv('operations-alignment.tsv', sep='\\t')\n",
    "else:\n",
    "    d = []    \n",
    "    for f in sra_df[0].unique():\n",
    "        a = !gcloud beta lifesciences pipelines run --pipeline-file={PIPELINE_JSON} --env-vars=INBUCKET={inbucket},OUTBUCKET={outbucket},SRA={f}\n",
    "        if len(a) == 1 and a[0].startswith('Running'):\n",
    "            a = a[0].replace('].','').split('/')[5]\n",
    "            d.append([f, a, 'running'])\n",
    "        else:\n",
    "            d.append([f, None, a])\n",
    "    operations_alignment['logs'] = {}\n",
    "    operations_alignment['operations'] = pandas.DataFrame(d, columns=['sra', 'id', 'status'])\n",
    "    operations_alignment['operations'].to_csv('operations-alignment.tsv', sep='\\t', index=None)\n",
    "\n",
    "display(operations_alignment['operations'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.chdir(op_dir)\n",
    "    \n",
    "df = operations_alignment['operations'].dropna()\n",
    "data = []                \n",
    "for i, r in df.iterrows():\n",
    "    id = r['id']\n",
    "    sample = '{}_{}'.format(r['sample'], r['sra'])\n",
    "    if os.path.exists('{}.json.gz'.format(sample)):\n",
    "        with gzip.GzipFile('{}.json.gz'.format(sample), 'r') as fin:  \n",
    "            operations_alignment['logs'][sample] = json.loads(fin.read().decode('utf-8'))\n",
    "    else:\n",
    "        if sample not in operations_alignment['logs']:\n",
    "            a = !gcloud beta lifesciences operations describe --format=json {id}\n",
    "            l = json.loads(''.join(a))\n",
    "            if 'done' in l:\n",
    "                operations_alignment['logs'][sample] = l\n",
    "                with gzip.GzipFile('{}.json.gz'.format(sample), 'w') as fout:   # 4. gzip\n",
    "                    fout.write(json.dumps(l, indent=2).encode('utf-8'))  \n",
    "    if sample in operations_alignment['logs']:\n",
    "        ts = get_gpc_starttimestamp(operations_alignment['logs'][sample])\n",
    "        ts = datetime.strptime(ts.split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        te = datetime.strptime(operations_alignment['logs'][sample]['metadata']['endTime'].split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        elapsed = te - ts\n",
    "        data.append([sample, elapsed])\n",
    "operations_alignment['gcp'] = pandas.DataFrame(data, columns=['Sample', 'Time'])\n",
    "operations_alignment['gcp']['Time'] = operations_alignment['gcp']['Time']/pandas.Timedelta('1 minute')\n",
    "\n",
    "MACHINE_PRICE = 0.16 # n1-standard-16 preemptible\n",
    "print('Alignment cost: $ {:.2f}'.format(operations_alignment['gcp']['Time'].sum() * MACHINE_PRICE/60))\n",
    "\n",
    "display(operations_alignment['gcp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download results from GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(result_dir)\n",
    "!gsutil -m -o 'GSUtil:parallel_composite_upload_threshold=150M' -o 'GSUtil:parallel_process_count=4' -o 'GSUtil:parallel_thread_count=4' cp -R gs://{outbucket}/ ./ \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reads Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(result_dir)\n",
    "\n",
    "str_msg = '| Sample | Total<br>reads | Mapped<br>reads | Mapped<br>reads (%) '\n",
    "str_msg += '| Un-Mapped<br>reads | Un-Mapped<br>reads (%) | Properly<br>paired<br>reads (%) '\n",
    "str_msg += '| Error<br>rate '\n",
    "str_msg += '| Average<br>length '\n",
    "str_msg += '| Average<br>quality '\n",
    "str_msg += '|\\n| --- | --- '\n",
    "str_msg += '| --- | --- '\n",
    "str_msg += '| --- '\n",
    "str_msg += '| --- '\n",
    "str_msg += '| --- '\n",
    "str_msg += '| --- '\n",
    "str_msg += '| --- '\n",
    "str_msg += '| --- '\n",
    "str_msg += '|\\n'\n",
    "for s in sra_df[0].unique():\n",
    "    alignment_path = os.path.join(outbucket, s)\n",
    "    str_msg += '| ' + s\n",
    "    str_msg += '| '\n",
    "    files = [f for ds, dr, files in os.walk(alignment_path) for f in files if\n",
    "             f.startswith(s) and f.endswith('.stats')\n",
    "             and os.path.getsize(os.path.join(alignment_path, f)) != 0]\n",
    "    if len(files) == 1:\n",
    "        f = os.path.relpath(os.path.join(alignment_path, files[0]))\n",
    "        stats = load_content_dict_line(f, ':', 'SN', '\\t', True, 'SN\\t', '')        \n",
    "        str_msg += \"{:,}\".format(int(stats['raw total sequences'])) + ' |'        \n",
    "        str_msg += \"{:,}\".format(int(stats['reads mapped'])) + ' |'\n",
    "        str_msg += \"{:.2f}\".format(float(stats['reads mapped'])*100/float(stats['raw total sequences'])) + ' |'\n",
    "        str_msg += \"{:,}\".format(int(stats['reads unmapped'])) + ' |'\n",
    "        str_msg += \"{:.2f}\".format(float(stats['reads unmapped'])*100/float(stats['raw total sequences'])) + ' |'\n",
    "        str_msg += \"{:,}\".format(float(stats['percentage of properly paired reads (%)'])) + ' |'\n",
    "        str_msg += \"{:.2e}\".format(float(stats['error rate'].replace('%', ''))) + ' |'\n",
    "        str_msg += \"{:,}\".format(int(stats['average length'].replace('%', ''))) + ' |'\n",
    "        str_msg += \"{:.1f}\".format(float(stats['average quality'].replace('%', ''))) + ' |'\n",
    "        str_msg += '\\n'\n",
    "    else:\n",
    "        str_msg += ' --- | --- | --- | --- |\\n'\n",
    "display(Markdown(str_msg))\n",
    "del str_msg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}