{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c481694",
   "metadata": {},
   "source": [
    "## Vector removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef35df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../config/init.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50bc48",
   "metadata": {},
   "source": [
    "### Loading data from {{ cookiecutter.dataset_name }}/sample_table.csv accession list\n",
    "\n",
    "The file `{{ cookiecutter.dataset_name }}/sample_table.cs` should contains a single column with all SRA IDs to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "KINGDOM = '{{ cookiecutter.kingdom }}'\n",
    "data_dir = os.path.join(DATA, DATASET)\n",
    "result_dir = working_dir(os.path.join(RESULTS, DATASET, 'vector_cleanup'))\n",
    "\n",
    "sra_df = pandas.read_csv(os.path.join(DATA, DATASET, 'sample_table.csv'), header=None)\n",
    "sra_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426f13b",
   "metadata": {},
   "source": [
    "## Testing gcloud configuration\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### [Cloud SDK](https://cloud.google.com/sdk)\n",
    "\n",
    "\n",
    "Run *gcloud init* to initialize the gcloud environment and follow its instructions:\n",
    "\n",
    " `$ gcloud init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "account = !gcloud config get-value account\n",
    "account = ''.join(account)\n",
    "project = !gcloud config get-value project\n",
    "project = ''.join(project)\n",
    "if account != '(unset)' and project != '(unset)':\n",
    "    print('Using account: {} with project: {}'.format(account, project))\n",
    "else:\n",
    "    print('Please, configure Cloud SDK before running this notebook')\n",
    "    print('Open a Terminal and run: gcloud init')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee3975",
   "metadata": {},
   "source": [
    "### Defining variables\n",
    "\n",
    "Edit GCP zone and region variable accordingly to your geographical location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98116c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-east4'\n",
    "ZONE = 'us-east4-c'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ab3b2",
   "metadata": {},
   "source": [
    "### Retrieve GCP storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_list = !gsutil ls\n",
    "bucket = None\n",
    "\n",
    "prefix = 'gs://{}-trimming-'.format(DATASET.lower())\n",
    "for l in bucket_list:\n",
    "    if prefix in l:\n",
    "        bucket = l.replace('gs://{}-trimming-'.format(DATASET.lower()),'').replace('/','')\n",
    "        break\n",
    "\n",
    "trimming_bucket = '{}-trimming-{}'.format(DATASET.lower(), bucket)\n",
    "print('in bucket: {}'.format(trimming_bucket))\n",
    "\n",
    "vector_bucket = '{}-vector-{}'.format(DATASET.lower(), bucket)\n",
    "\n",
    "bucket_list = !gsutil ls gs://{vector_bucket}\n",
    "if ''.join(bucket_list).startswith('BucketNotFoundException'):\n",
    "    !gsutil mb gs://{vector_bucket}\n",
    "\n",
    "print('vector bucket: {}'.format(vector_bucket))\n",
    "\n",
    "for s in sra_df[0].unique():\n",
    "    out_bucket = '{}-{}'.format(s.lower(),bucket)\n",
    "\n",
    "    bucket_list = !gsutil ls gs://{out_bucket}\n",
    "    if ''.join(bucket_list).startswith('BucketNotFoundException'):\n",
    "        !gsutil mb gs://{out_bucket}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40137047",
   "metadata": {},
   "source": [
    "### Submitting jobs for vector cleanup  to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = {\n",
    "    'logs':{},\n",
    "    'operations': pandas.DataFrame(columns=['sample', 'id', 'status'])}\n",
    "PIPELINE = os.path.join(BIN, 'gcp', 'pipeline-transcriptome-fastq-vector-cleanup.json')\n",
    "\n",
    "os.chdir(result_dir)\n",
    "op_dir = os.path.join(result_dir, 'gcp')\n",
    "if not os.path.exists(op_dir):\n",
    "    os.mkdir(op_dir)\n",
    "os.chdir(op_dir)\n",
    "\n",
    "if os.path.exists('operations-vector-fastq.tsv'):\n",
    "    operations['operations'] = pandas.read_csv('operations-vector-fastq.tsv', sep='\\t')\n",
    "\n",
    "d = []\n",
    "for f in sra_df[0].unique():\n",
    "    if f not in operations['operations']['sample'].unique():\n",
    "        print('Submitting sample: ' + f)\n",
    "        a = !gcloud beta lifesciences pipelines run --pipeline-file={PIPELINE} --env-vars=OUTBUCKET={vector_bucket},INBUCKET={trimming_bucket},SAMPLE={f}\n",
    "        if len(a) == 1 and a[0].startswith('Running'):\n",
    "            a = a[0].replace('].','').split('/')[5]\n",
    "            d.append([f, a, 'running'])\n",
    "        else:\n",
    "            d.append([f, None, a])\n",
    "\n",
    "if d:\n",
    "    operations['operations'] = pandas.concat([operations['operations'], pandas.DataFrame(d, columns=['sample', 'id', 'status'])])\n",
    "    operations['operations'].to_csv('operations-vector-fastq.tsv', sep='\\t', index=None)\n",
    "\n",
    "display(operations['operations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee264b",
   "metadata": {},
   "source": [
    "### Checking GCP runs\n",
    "This cell will download the GCP logs for completed operations (jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900cee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(op_dir)\n",
    "\n",
    "df = operations['operations'].dropna()\n",
    "data = []\n",
    "with_error = []\n",
    "count_running = 0\n",
    "for i, r in df.iterrows():\n",
    "    id = r['id']\n",
    "    if os.path.exists('{}.json.gz'.format(r['sample'])):\n",
    "        with gzip.GzipFile('{}.json.gz'.format(r['sample']), 'r') as fin:\n",
    "            operations['logs'][r['sample']] = json.loads(fin.read().decode('utf-8'))\n",
    "    else:\n",
    "        if r['sample'] not in operations['logs']:\n",
    "            a = !gcloud beta lifesciences operations describe --format=json {id}\n",
    "            l = json.loads(''.join(a))\n",
    "            if 'done' in l:\n",
    "                if 'error' not in l:\n",
    "                    operations['logs'][r['sample']] = l\n",
    "                    with gzip.GzipFile('{}.json.gz'.format(r['sample']), 'w') as fout:   # 4. gzip\n",
    "                        fout.write(json.dumps(l, indent=2).encode('utf-8'))\n",
    "                else:\n",
    "                    with_error.append(id)\n",
    "            else:\n",
    "                count_running += 1\n",
    "    if r['sample'] in operations['logs']:\n",
    "        ts = get_gpc_starttimestamp(operations['logs'][r['sample']])\n",
    "        ts = datetime.strptime(ts.split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        te = datetime.strptime(operations['logs'][r['sample']]['metadata']['endTime'].split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        elapsed = te - ts\n",
    "        data.append([r['sample'], elapsed])\n",
    "\n",
    "if count_running != 0:\n",
    "    print('Still running {}'.format(count_running))\n",
    "\n",
    "if with_error:\n",
    "    operations['operations'] = df[~df['id'].isin(with_error)]\n",
    "    operations['operations'].to_csv('operations-vector-fastq.tsv', sep='\\t', index=None)\n",
    "    print('{} runs with errors. Please rerun previous cell'.format(len(with_error)))\n",
    "elif count_running == 0:\n",
    "    operations['gcp'] = pandas.DataFrame(data, columns=['Sample', 'Time'])\n",
    "    operations['gcp']['Time'] = operations['gcp']['Time']/pandas.Timedelta('1 minute')\n",
    "    display(operations['gcp'])\n",
    "\n",
    "    MACHINE_PRICE = 0.16 # n1-standard-16 preemptible\n",
    "    print('Computig cost: $ {:.2f}'.format(operations['gcp']['Time'].sum() * MACHINE_PRICE/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519bbfc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Submitting jobs for contamination screening on fastq files\n",
    "\n",
    "Edit variable TOTAL_PER_FILE accordingly with the number of reads in your fastq files. This will use to run\n",
    "multiple blast searches in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "operations = {\n",
    "    'logs':{},\n",
    "    'operations': pandas.DataFrame(columns=['sample', 'id', 'status'])}\n",
    "TOTAL_PER_FILE = 1000000\n",
    "PIPELINE = os.path.join(BIN, 'gcp', 'pipeline-transcriptome-fastq-contamination-cleanup.json')\n",
    "\n",
    "os.chdir(result_dir)\n",
    "op_dir = os.path.join(result_dir, 'gcp')\n",
    "if not os.path.exists(op_dir):\n",
    "    os.mkdir(op_dir)\n",
    "os.chdir(op_dir)\n",
    "\n",
    "if os.path.exists('operations-fastq-contamination-cleanup.tsv'):\n",
    "    operations['operations'] = pandas.read_csv('operations-fastq-contamination-cleanup.tsv', sep='\\t')\n",
    "\n",
    "d = []\n",
    "for f in sra_df[0].unique():\n",
    "    if f not in operations['operations']['sample'].unique():\n",
    "        out_bucket = '{}-{}'.format(f.lower(),bucket)\n",
    "        a = !gcloud beta lifesciences pipelines run --pipeline-file={PIPELINE} --env-vars=OUTBUCKET={out_bucket},INBUCKET={vector_bucket},SAMPLE={f},TOTAL_PER_FILE={TOTAL_PER_FILE},PERCENT_IDENTITY={PERCENT_IDENTITY}\n",
    "        if len(a) == 1 and a[0].startswith('Running'):\n",
    "            a = a[0].replace('].','').split('/')[5]\n",
    "            d.append([f, a, 'running'])\n",
    "        else:\n",
    "            d.append([f, None, a])\n",
    "if d:\n",
    "    operations['operations'] = pandas.concat([operations['operations'], pandas.DataFrame(d, columns=['sample', 'id', 'status'])])\n",
    "    operations['operations'].to_csv('operations-fastq-contamination-cleanup.tsv', sep='\\t', index=None)\n",
    "\n",
    "display(operations['operations'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking GCP runs\n",
    "This cell will download the GCP logs for completed operations (jobs)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(op_dir)\n",
    "\n",
    "df = operations['operations'].dropna()\n",
    "data = []\n",
    "with_error = []\n",
    "count_running = 0\n",
    "for i, r in df.iterrows():\n",
    "    id = r['id']\n",
    "    if os.path.exists('{}_cont.json.gz'.format(r['sample'])):\n",
    "        with gzip.GzipFile('{}_cont.json.gz'.format(r['sample']), 'r') as fin:\n",
    "            operations['logs'][r['sample']] = json.loads(fin.read().decode('utf-8'))\n",
    "    else:\n",
    "        if r['sample'] not in operations['logs']:\n",
    "            a = !gcloud beta lifesciences operations describe --format=json {id}\n",
    "            l = json.loads(''.join(a))\n",
    "            if 'done' in l:\n",
    "                if 'error' not in l:\n",
    "                    operations['logs'][r['sample']] = l\n",
    "                    with gzip.GzipFile('{}_cont.json.gz'.format(r['sample']), 'w') as fout:   # 4. gzip\n",
    "                        fout.write(json.dumps(l, indent=2).encode('utf-8'))\n",
    "                else:\n",
    "                    with_error.append(id)\n",
    "            else:\n",
    "                count_running += 1\n",
    "    if r['sample'] in operations['logs']:\n",
    "        ts = get_gpc_starttimestamp(operations['logs'][r['sample']])\n",
    "        ts = datetime.strptime(ts.split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        te = datetime.strptime(operations['logs'][r['sample']]['metadata']['endTime'].split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        elapsed = te - ts\n",
    "        data.append([r['sample'], elapsed])\n",
    "\n",
    "if count_running != 0:\n",
    "    print('Still running {}'.format(count_running))\n",
    "\n",
    "if with_error:\n",
    "    operations['operations'] = df[~df['id'].isin(with_error)]\n",
    "    operations['operations'].to_csv('operations-fastq-contamination-cleanup.tsv', sep='\\t', index=None)\n",
    "    print('{} runs with errors. Please rerun previous cell'.format(len(with_error)))\n",
    "elif count_running == 0:\n",
    "    operations['gcp'] = pandas.DataFrame(data, columns=['Sample', 'Time'])\n",
    "    operations['gcp']['Time'] = operations['gcp']['Time']/pandas.Timedelta('1 minute')\n",
    "    display(operations['gcp'])\n",
    "    MACHINE_PRICE = 0.16 # n1-standard-16 preemptible\n",
    "    print('Computig cost: $ {:.2f}'.format(operations['gcp']['Time'].sum() * MACHINE_PRICE/60))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Screening to identify foreign chromosomes contamination"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "foreign_databases = ['archaea','other_eukaryota', 'arthropoda', 'fungi', 'chordata',\n",
    "                     'other_metazoa', 'viruses_and_viroids', 'viridiplantae', 'bacteria']\n",
    "\n",
    "PERCENT_IDENTITY = 95.0\n",
    "operations = {\n",
    "    'logs':{},\n",
    "    'operations': pandas.DataFrame(columns=['sample', 'db', 'file', 'id', 'status'])}\n",
    "PIPELINE = os.path.join(BIN, 'gcp', 'pipeline-transcriptome-fastq-foreign-contamination-blastn.json')\n",
    "BLASTBUCKET = 'contamination-foreign-screen'\n",
    "\n",
    "os.chdir(result_dir)\n",
    "op_dir = os.path.join(result_dir, 'gcp')\n",
    "if not os.path.exists(op_dir):\n",
    "    os.mkdir(op_dir)\n",
    "os.chdir(op_dir)\n",
    "\n",
    "if os.path.exists('operations-fastq-foreign-contamination-blastn.tsv'):\n",
    "    operations['operations'] = pandas.read_csv('operations-fastq-foreign-contamination-blastn.tsv', sep='\\t')\n",
    "\n",
    "d = []\n",
    "df = operations['operations']\n",
    "for f in sra_df[0].unique():\n",
    "    out_bucket = '{}-{}'.format(f.lower(),bucket)\n",
    "    files = !gsutil ls gs://{out_bucket}/{TOTAL_PER_FILE}_*.fsa.gz\n",
    "    for file in files:\n",
    "        file = os.path.basename(file).replace('.fsa.gz', '')\n",
    "        for db in foreign_databases:\n",
    "            if df[(df['sample'] == f) & (df['db'] == db) & (df['file'] == file)].empty:\n",
    "                print('Submitting sample: {} {} {}'.format(f, db, file))\n",
    "                a = !gcloud beta lifesciences pipelines run --pipeline-file={PIPELINE} --env-vars=OUTBUCKET={out_bucket},INBUCKET={out_bucket},SAMPLE={file},DB={db},BLASTBUCKET={BLASTBUCKET},PERCENT_IDENTITY={PERCENT_IDENTITY}\n",
    "                if len(a) == 1 and a[0].startswith('Running'):\n",
    "                    a = a[0].replace('].','').split('/')[5]\n",
    "                    d.append([f, db, file, a, 'running'])\n",
    "                else:\n",
    "                    d.append([f, db, file, None, a])\n",
    "if d:\n",
    "    operations['operations'] = pandas.concat([operations['operations'], pandas.DataFrame(d, columns=['sample', 'db', 'file', 'id', 'status'])])\n",
    "    operations['operations'].to_csv('operations-fastq-foreign-contamination-blastn.tsv', sep='\\t', index=None)\n",
    "\n",
    "display(operations['operations'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking GCP runs\n",
    "This cell will download the GCP logs for completed operations (jobs)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(op_dir)\n",
    "\n",
    "df = operations['operations'].dropna()\n",
    "data = []\n",
    "with_error = []\n",
    "count_running = 0\n",
    "for i, r in df.iterrows():\n",
    "    id = r['id']\n",
    "    key = '{}_{}_{}'.format(r['sample'], r['db'], r['file'])\n",
    "    if os.path.exists('{}_foreign_screen.json.gz'.format(key)):\n",
    "        with gzip.GzipFile('{}_foreign_screen.json.gz'.format(key), 'r') as fin:\n",
    "            operations['logs'][key] = json.loads(fin.read().decode('utf-8'))\n",
    "    else:\n",
    "        if key not in operations['logs']:\n",
    "            a = !gcloud beta lifesciences operations describe --format=json {id}\n",
    "            l = json.loads(''.join(a))\n",
    "            if 'done' in l:\n",
    "                if 'error' not in l:\n",
    "                    operations['logs'][key] = l\n",
    "                    with gzip.GzipFile('{}_foreign_screen.json.gz'.format(key), 'w') as fout:   # 4. gzip\n",
    "                        fout.write(json.dumps(l, indent=2).encode('utf-8'))\n",
    "                else:\n",
    "                    with_error.append(id)\n",
    "            else:\n",
    "                count_running += 1\n",
    "    if key in operations['logs']:\n",
    "        ts = get_gpc_starttimestamp(operations['logs'][key])\n",
    "        ts = datetime.strptime(ts.split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        te = datetime.strptime(operations['logs'][key]['metadata']['endTime'].split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        elapsed = te - ts\n",
    "        data.append([key, elapsed])\n",
    "\n",
    "if count_running != 0:\n",
    "    print('Still running {}'.format(count_running))\n",
    "\n",
    "if with_error:\n",
    "    operations['operations'] = df[~df['id'].isin(with_error)]\n",
    "    operations['operations'].to_csv('operations-fastq-foreign-contamination-blastn.tsv', sep='\\t', index=None)\n",
    "    print('{} runs with errors. Please rerun previous cell'.format(len(with_error)))\n",
    "elif count_running == 0:\n",
    "    operations['gcp'] = pandas.DataFrame(data, columns=['Run', 'Time'])\n",
    "    operations['gcp']['Time'] = operations['gcp']['Time']/pandas.Timedelta('1 minute')\n",
    "    display(operations['gcp'])\n",
    "\n",
    "    MACHINE_PRICE = 0.16 # n1-standard-16 preemptible\n",
    "    print('Computig cost: $ {:.2f}'.format(operations['gcp']['Time'].sum() * MACHINE_PRICE/60))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing contaminated reads from fastq files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "operations = {\n",
    "    'logs':{},\n",
    "    'operations': pandas.DataFrame(columns=['sample', 'id', 'status'])}\n",
    "PIPELINE = os.path.join(BIN, 'gcp', 'pipeline-transcriptome-fastq-foreign-contamination-cleanup.json')\n",
    "\n",
    "os.chdir(result_dir)\n",
    "op_dir = os.path.join(result_dir, 'gcp')\n",
    "if not os.path.exists(op_dir):\n",
    "    os.mkdir(op_dir)\n",
    "os.chdir(op_dir)\n",
    "\n",
    "if os.path.exists('operations-fastq-foreign-contamination-cleanup.tsv'):\n",
    "    operations['operations'] = pandas.read_csv('operations-fastq-foreign-contamination-cleanup.tsv', sep='\\t')\n",
    "\n",
    "d = []\n",
    "df = operations['operations']\n",
    "for f in sra_df[0].unique():\n",
    "    blast_bucket = '{}-{}'.format(f.lower(),bucket)\n",
    "    if f not in operations['operations']['sample'].unique():\n",
    "        print('Submitting sample: {}'.format(f))\n",
    "        a = !gcloud beta lifesciences pipelines run --pipeline-file={PIPELINE} --env-vars=OUTBUCKET={vector_bucket},BLAST_RESULTS={blast_bucket},SAMPLE={f},FASTQ_BUCKET={vector_bucket},KINGDOM={KINGDOM},TOTAL_PER_FILE={TOTAL_PER_FILE}\n",
    "        if len(a) == 1 and a[0].startswith('Running'):\n",
    "            a = a[0].replace('].','').split('/')[5]\n",
    "            d.append([f, a, 'running'])\n",
    "        else:\n",
    "            d.append([f, None, a])\n",
    "if d:\n",
    "    operations['operations'] = pandas.concat([operations['operations'], pandas.DataFrame(d, columns=['sample', 'id', 'status'])])\n",
    "    operations['operations'].to_csv('operations-fastq-foreign-contamination-cleanup.tsv', sep='\\t', index=None)\n",
    "\n",
    "display(operations['operations'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking GCP runs\n",
    "This cell will download the GCP logs for completed operations (jobs)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(op_dir)\n",
    "\n",
    "df = operations['operations'].dropna()\n",
    "data = []\n",
    "with_error = []\n",
    "count_running = 0\n",
    "for i, r in df.iterrows():\n",
    "    id = r['id']\n",
    "    if os.path.exists('{}_foreign_cleanup.json.gz'.format(r['sample'])):\n",
    "        with gzip.GzipFile('{}_foreign_cleanup.json.gz'.format(r['sample']), 'r') as fin:\n",
    "            operations['logs'][r['sample']] = json.loads(fin.read().decode('utf-8'))\n",
    "    else:\n",
    "        if r['sample'] not in operations['logs']:\n",
    "            a = !gcloud beta lifesciences operations describe --format=json {id}\n",
    "            l = json.loads(''.join(a))\n",
    "            if 'done' in l:\n",
    "                if 'error' not in l:\n",
    "                    operations['logs'][r['sample']] = l\n",
    "                    with gzip.GzipFile('{}_foreign_cleanup.json.gz'.format(r['sample']), 'w') as fout:   # 4. gzip\n",
    "                        fout.write(json.dumps(l, indent=2).encode('utf-8'))\n",
    "                else:\n",
    "                    with_error.append(id)\n",
    "            else:\n",
    "                count_running += 1\n",
    "    if r['sample'] in operations['logs']:\n",
    "        ts = get_gpc_starttimestamp(operations['logs'][r['sample']])\n",
    "        ts = datetime.strptime(ts.split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        te = datetime.strptime(operations['logs'][r['sample']]['metadata']['endTime'].split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        elapsed = te - ts\n",
    "        data.append([r['sample'], elapsed])\n",
    "\n",
    "if count_running != 0:\n",
    "    print('Still running {}'.format(count_running))\n",
    "\n",
    "if with_error:\n",
    "    operations['operations'] = df[~df['id'].isin(with_error)]\n",
    "    operations['operations'].to_csv('operations-fastq-foreign-contamination-cleanup.tsv', sep='\\t', index=None)\n",
    "    print('{} runs with errors. Please rerun previous cell'.format(len(with_error)))\n",
    "elif count_running == 0:\n",
    "    operations['gcp'] = pandas.DataFrame(data, columns=['Sample', 'Time'])\n",
    "    operations['gcp']['Time'] = operations['gcp']['Time']/pandas.Timedelta('1 minute')\n",
    "    display(operations['gcp'])\n",
    "    MACHINE_PRICE = 0.16 # n1-standard-16 preemptible\n",
    "    print('Computig cost: $ {:.2f}'.format(operations['gcp']['Time'].sum() * MACHINE_PRICE/60))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}