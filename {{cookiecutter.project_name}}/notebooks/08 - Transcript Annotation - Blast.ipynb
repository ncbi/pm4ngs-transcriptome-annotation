{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the BlastP results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../config/init.py\n",
    "import itertools\n",
    "from goenrichment.go import parse_go_obo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Taxonomy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(DATA, 'taxonomy', 'taxonomy_networkx.pickle')\n",
    "tax = pickle.load(open(pickle_file, \"rb\"))\n",
    "viridiplantae = [int(i) for i in successors('33090', tax)]  \n",
    "print('{} taxonomies IDs in the list'.format(len(viridiplantae)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading GO network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading gene_info\n",
    "if not os.path.exists(os.path.join(DATA, 'go-basic.obo')):\n",
    "    os.chdir(DATA)\n",
    "    !wget http://current.geneontology.org/ontology/go-basic.obo\n",
    "go = nx.DiGraph()\n",
    "entries = parse_go_obo(os.path.join(DATA, 'go-basic.obo'))\n",
    "nodes, edges = zip(*entries)\n",
    "go.add_nodes_from(nodes)\n",
    "go.add_edges_from(itertools.chain.from_iterable(edges))\n",
    "go.graph['roots'] = {data['name']: n for n, data in go._node.items() \\\n",
    "                     if 'name' in data and data['name'] == data['namespace']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading flat databases for cross-referencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the UniProt ID mapping to link GO with Protein GI\n",
    "if not os.path.exists(os.path.join(DATA, 'idmapping_selected.pkl')):\n",
    "    if not os.path.exists(os.path.join(DATA, 'idmapping_selected.tab.gz')):\n",
    "        os.chdir(DATA)\n",
    "        !wget ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/idmapping_selected.tab.gz\n",
    "\n",
    "    # Total number of lines while writing this code. Use it as a rough estimate \n",
    "    total_lines = 181252700 \n",
    "    bar_length = 100\n",
    "    idmapping = pandas.DataFrame()\n",
    "    total = 0\n",
    "    for chunk in pandas.read_csv(os.path.join(DATA, 'idmapping_selected.tab.gz'), header=None, sep='\\t', chunksize=10000, low_memory=False):    \n",
    "        total += len(chunk)\n",
    "        progress = total * 100/total_lines\n",
    "        i = int(progress)\n",
    "        chunk = chunk[(chunk[12].isin(viridiplantae)) & (chunk[2].notnull()) & (chunk[4].notnull())]        \n",
    "        if not chunk.empty:\n",
    "            chunk = chunk[[2, 3, 6, 7, 8, 9, 12]]\n",
    "            chunk = chunk.drop_duplicates()\n",
    "            idmapping = pandas.concat([idmapping, chunk])\n",
    "        print('{0}/{1} [{2}] {3:.1f}%'.format(len(idmapping), total, \"#\" * i + \"-\" * (bar_length - i), progress), end='\\r')\n",
    "    del chunk\n",
    "    print()\n",
    "    idmapping = idmapping.rename(columns={2:'GeneID', 3:'PAs', 6:'GOES', 7:'UniRef100', 8:'UniRef90', 9:'UniRef50', 12:'#tax_id'})\n",
    "    idmapping = idmapping.assign(PAs=idmapping.PAs.astype(str))\n",
    "    idmapping = idmapping.assign(GOES=idmapping.GOES.astype(str))\n",
    "    df = pandas.DataFrame(idmapping.PAs.str.split(';').tolist(), index=idmapping.GeneID).stack()\n",
    "    df = df.reset_index([0, 'GeneID'])\n",
    "    df = df.rename(columns={0:'protein_accession.version'})\n",
    "    idmapping = idmapping.merge(df, on='GeneID')    \n",
    "    new = idmapping['protein_accession.version'].str.split(\".\", n = 1, expand = True) \n",
    "    idmapping[\"protein_accession\"] = new[0]\n",
    "    idmapping = idmapping.drop(['PAs', 'protein_accession.version'], axis=1)\n",
    "    df = pandas.DataFrame(idmapping.GOES.str.split(';').tolist(), index=idmapping.GeneID).stack()\n",
    "    df = df.reset_index([0, 'GeneID'])\n",
    "    df = df.rename(columns={0:'GO'})\n",
    "    idmapping = idmapping.merge(df, on='GeneID')\n",
    "    idmapping = idmapping.drop(['GOES'], axis=1)\n",
    "    idmapping = idmapping[idmapping['GO'].str.startswith('GO:')]       \n",
    "    idmapping = idmapping.drop_duplicates()    \n",
    "    idmapping = idmapping.reset_index(drop=True)\n",
    "    del df\n",
    "    idmapping.to_pickle(os.path.join(DATA, 'idmapping_selected.pkl'))\n",
    "else:\n",
    "    idmapping = pandas.read_pickle(os.path.join(DATA, 'idmapping_selected.pkl'))\n",
    "    print('idmapping: {}'.format(len(idmapping)))\n",
    "\n",
    "display(idmapping.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading ec2go map\n",
    "if not os.path.exists(os.path.join(DATA, 'ec2go')):\n",
    "    os.chdir(DATA)\n",
    "    !wget http://geneontology.org/external2go/ec2go\n",
    "ec2go = pandas.read_csv(os.path.join(DATA, 'ec2go'), sep=' \\> ', comment='!', header=None, engine='python')\n",
    "new = ec2go[1].str.split(\" ; \", n = 1, expand = True)\n",
    "ec2go[1] = new[1]\n",
    "ec2go.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading gene2go\n",
    "if not os.path.exists(os.path.join(DATA, 'gene2go.gz')):\n",
    "    os.chdir(DATA)\n",
    "    !wget https://ftp.ncbi.nlm.nih.gov/gene/DATA/gene2go.gz\n",
    "gene2go = pandas.read_csv(os.path.join(DATA, 'gene2go.gz'), sep='\\t')\n",
    "t = len(gene2go)\n",
    "gene2go = gene2go[gene2go['#tax_id'].isin(viridiplantae)]\n",
    "f = len(gene2go)\n",
    "print('{}/{} gene2go terms in viridiplantae'.format(f,t))\n",
    "\n",
    "display(gene2go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(DATA, 'gene2accession.pkl')):\n",
    "    if not os.path.exists(os.path.join(DATA, 'gene2accession.gz')):\n",
    "        os.chdir(DATA)\n",
    "        !wget https://ftp.ncbi.nlm.nih.gov/gene/DATA/gene2accession.gz\n",
    "\n",
    "    gene2accession = pandas.DataFrame()\n",
    "    for chunk in pandas.read_csv(os.path.join(DATA, 'gene2accession.gz'), sep='\\t', chunksize=10000, low_memory=False):    \n",
    "        chunk = chunk[chunk['#tax_id'].isin(viridiplantae)]\n",
    "        if not chunk.empty:\n",
    "            chunk = chunk[['#tax_id', 'GeneID', 'protein_accession.version','protein_gi', 'genomic_nucleotide_accession.version','genomic_nucleotide_gi']]\n",
    "            chunk = chunk.drop_duplicates()\n",
    "            gene2accession = pandas.concat([gene2accession, chunk])\n",
    "            print('gene2accession: {}'.format(len(gene2accession)), end='\\r')\n",
    "    del chunk    \n",
    "    print()\n",
    "    gene2accession.to_pickle(os.path.join(DATA, 'gene2accession.pkl'))\n",
    "else:\n",
    "    gene2accession = pandas.read_pickle(os.path.join(DATA, 'gene2accession.pkl'))\n",
    "    print('gene2accession: {}'.format(len(gene2accession)))\n",
    "display(gene2accession.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Transcripts fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPTOME_FILE = os.path.join(RESULTS, DATASET, 'trinity_assembly', 'Trinity.fasta.gz')\n",
    "data = []\n",
    "with gzip.open(TRANSCRIPTOME_FILE, \"rt\") as handle:\n",
    "    for record in SeqIO.parse(handle, \"fasta\"):\n",
    "        data.append([record.id, len(record.seq)])\n",
    "trans_len = pandas.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_len.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading sample list from GCP operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pandas.read_csv(os.path.join(RESULTS, DATASET, 'annotation','gcp', 'operations.tsv'), sep='\\t')\n",
    "samples = samples['sample']\n",
    "bar_length = len(samples)\n",
    "print('{} fasta files to annotate'.format(len(samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlastP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "THREADS = 25\n",
    "MIN_BITSCORE = 100\n",
    "\n",
    "def worker(s):\n",
    "    \"\"\"worker function\"\"\"    \n",
    "    dna = os.path.join(RESULTS, DATASET, 'annotation', 'blasts', s, s + '_nocont.fsa.gz')\n",
    "    prot = os.path.join(RESULTS, DATASET, 'annotation', 'blasts', s, s + '_nocont_transdecoder.fsa.gz')     \n",
    "    fasta = []\n",
    "    with gzip.open(dna, \"rt\") as dna_handle:\n",
    "        for dna_record in SeqIO.parse(dna_handle, \"fasta\"):\n",
    "            if len(dna_record.seq) >= 200:\n",
    "                fasta.append(dna_record.id)\n",
    "    df_blast_p = pandas.read_csv(os.path.join(RESULTS, DATASET, 'annotation', 'blasts', s, s + '_nocont_transdecoder_blastp.tsv.gz'), sep='\\t', header=None)\n",
    "    df_blast_p = df_blast_p[df_blast_p[4] >= MIN_BITSCORE]   # Filtering by bitscore bigger than 50\n",
    "    df_blast_p = df_blast_p[df_blast_p[6].isin(viridiplantae)]\n",
    "    new = df_blast_p[0].str.split(\".\", n = 1, expand = True)\n",
    "    df_blast_p[0] = new[0]\n",
    "    df_blast_p[7] = new[1]\n",
    "    df_blast_p = df_blast_p.drop_duplicates(keep='first')  \n",
    "    df_blast_p = df_blast_p[df_blast_p[0].isin(fasta)]\n",
    "    \n",
    "    d = df_blast_p[[0,7]].drop_duplicates(keep='first')\n",
    "    d = d.groupby(0).count().reset_index()\n",
    "    trans_with_multiple_proteins = d[d[7] > 1][0].unique()\n",
    "    df_blast_p_with_multiple_proteins = df_blast_p[df_blast_p[0].isin(trans_with_multiple_proteins)]\n",
    "    df_blast_p = df_blast_p[~df_blast_p[0].isin(trans_with_multiple_proteins)]\n",
    "    \n",
    "    prots = {}     \n",
    "    with gzip.open(prot, \"rt\") as prot_handle:          \n",
    "        for prot_record in SeqIO.parse(prot_handle, \"fasta\"):\n",
    "            f = prot_record.id.split('.')\n",
    "            if f[0] in trans_with_multiple_proteins:\n",
    "                p = prots.setdefault(f[0], {})   \n",
    "                d = prot_record.description.split(' ')[1][5:]\n",
    "                b = df_blast_p_with_multiple_proteins[(df_blast_p_with_multiple_proteins[0] == f[0])&(df_blast_p_with_multiple_proteins[7] == f[1])]                 \n",
    "                if not p:                    \n",
    "                    p['d'] = d\n",
    "                    p['b'] = b\n",
    "                else:  \n",
    "                    replace = True if p['d'] != 'complete' and d == 'complete' else False\n",
    "                    if not replace and p['d'] != 'complete':\n",
    "                        replace = True if len(p['b']) < len(b) else False                         \n",
    "                    if replace:     \n",
    "                        prots[f[0]]['d'] = d\n",
    "                        prots[f[0]]['b'] = b\n",
    "    for k, p in prots.items():\n",
    "        df_blast_p = pandas.concat([df_blast_p, p['b']])\n",
    "    return {'df': df_blast_p, 'seq': len(fasta)}\n",
    "\n",
    "# Submitting all samples as jobs\n",
    "total_seq = 0\n",
    "blastp = pandas.DataFrame()\n",
    "p = Pool(processes=THREADS)\n",
    "data = p.map(worker, [s for s in samples])\n",
    "for d in data:\n",
    "    blastp = pandas.concat([blastp, d['df']])\n",
    "    total_seq += d['seq']\n",
    "p.close()\n",
    "print('Total sequences: {}\\nBlastP hits: {}'.format(total_seq, len(blastp)))\n",
    "blastp.to_csv(os.path.join(RESULTS, DATASET, 'annotation', 'blastp.tsv.gz'), header=None, sep='\\t', index=None, compression='gzip')\n",
    "display(blastp.head())\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geneID2ProtGI = gene2accession[['GeneID','protein_accession.version']].drop_duplicates()\n",
    "geneID2ProtGI = geneID2ProtGI[geneID2ProtGI['protein_accession.version'] != '-']\n",
    "geneID2ProtGI = gene2go.merge(geneID2ProtGI, on='GeneID')\n",
    "geneID2ProtGI = geneID2ProtGI[['GO_ID', 'PubMed', '#tax_id', 'protein_accession.version']].drop_duplicates()\n",
    "new = geneID2ProtGI[\"protein_accession.version\"].str.split(\".\", n = 1, expand = True) \n",
    "geneID2ProtGI['protein_accession'] = new[0]\n",
    "geneID2ProtGI = geneID2ProtGI[['GO_ID', 'PubMed', '#tax_id', 'protein_accession']].drop_duplicates()\n",
    "\n",
    "d = blastp[[0,2,7]].drop_duplicates()\n",
    "new = d[2].str.split(\".\", n = 1, expand = True) \n",
    "d[1] = new[0]\n",
    "d = d[[0, 1, 7]].drop_duplicates()\n",
    "trans2go_blastp = d.merge(geneID2ProtGI, left_on=1, right_on='protein_accession')\n",
    "trans2go_blastp = trans2go_blastp[[0,'GO_ID', 'PubMed', '#tax_id', 7]]\n",
    "trans2go_blastp = trans2go_blastp.drop_duplicates()\n",
    "print('\\n{} pair transcript GO term from NCBI Gene'.format(len(trans2go_blastp)))\n",
    "print('{} transcript with GO term from NCBI Gene'.format(len(trans2go_blastp[0].unique())))\n",
    "trans2go_blastp.to_csv(os.path.join(RESULTS, DATASET, 'blastp_go_gene.tsv.gz'), header=None, sep='\\t', index=None, compression='gzip')\n",
    "\n",
    "trans2go_uniprot = d.merge(idmapping, left_on=1, right_on='protein_accession')\n",
    "trans2go_uniprot = trans2go_uniprot[[0,'GO', 'UniRef100', 'UniRef90', 'UniRef50', '#tax_id', 7]]\n",
    "trans2go_uniprot['GO'] = trans2go_uniprot['GO'].str.strip()\n",
    "trans2go_uniprot = trans2go_uniprot.drop_duplicates()\n",
    "print('\\n{} pair transcript GO term from Uniprot'.format(len(trans2go_uniprot)))\n",
    "print('{} transcript with GO term from Uniprot'.format(len(trans2go_uniprot[0].unique())))\n",
    "trans2go_uniprot.to_csv(os.path.join(RESULTS, DATASET, 'blastp_go_uniprot.tsv.gz'), header=None, sep='\\t', index=None, compression='gzip')\n",
    "\n",
    "del d\n",
    "del geneID2ProtGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = trans2go_uniprot[[0,'GO', 7]]\n",
    "df1 = df1.drop_duplicates()\n",
    "df2 = trans2go_blastp[[0,'GO_ID', 7]]\n",
    "df2 = df2.drop_duplicates()\n",
    "df2 = df2.rename(columns={'GO_ID':'GO'})\n",
    "df3 = pandas.concat([df1, df2])\n",
    "df3 = df3.drop_duplicates()\n",
    "print('\\n{} pair transcript GO term'.format(len(df3)))\n",
    "print('{} transcript with GO term'.format(len(df3[0].unique())))\n",
    "display(df3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "def predecessors(g, O):\n",
    "    \"\"\"\n",
    "    Extract predecessors nodes from an starting node\n",
    "    :param g: starting node name\n",
    "    :param O: Graph\n",
    "    :return: a set with node names\n",
    "    \"\"\"\n",
    "    result = {g}\n",
    "    for o in O.predecessors(g):\n",
    "        result.update(predecessors(o, O))\n",
    "    return result\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def worker(t_list):\n",
    "    res = pandas.DataFrame()\n",
    "    for t in t_list:\n",
    "        df_go = df3[df3[0] == t]    \n",
    "        l = df_go.GO.unique()    \n",
    "        to_del = set()\n",
    "        for i in range(0, len(l) - 1):\n",
    "            try:\n",
    "                si = [o for o in predecessors(l[i], go)]\n",
    "                for j in range(i + 1, len(l)):\n",
    "                    try:\n",
    "                        sj = [o for o in predecessors(l[j], go)]        \n",
    "                        g = go.subgraph(si).subgraph(sj)\n",
    "                        if len(si) < len(sj) and len(g) == len(si):\n",
    "                            to_del.add(l[i])\n",
    "                        elif len(si) > len(sj) and len(g) == len(sj):\n",
    "                            to_del.add(l[j])\n",
    "                    except nx.exception.NetworkXError:\n",
    "                        to_del.add(l[j])\n",
    "            except nx.exception.NetworkXError:\n",
    "                to_del.add(l[i])\n",
    "        if to_del:\n",
    "            df_go = df_go[~df_go['GO'].isin(to_del)]\n",
    "        res = pandas.concat([res, df_go])    \n",
    "    return res\n",
    "\n",
    "# Submitting all samples as jobs\n",
    "p = Pool(processes=THREADS)\n",
    "data = p.map(worker, [d for d in list(chunks(df3[0].unique(), 1500))])\n",
    "p.close()\n",
    "blast_go = pandas.DataFrame()\n",
    "for d in data:\n",
    "    blast_go = pandas.concat([blast_go, d])\n",
    "\n",
    "blast_go = blast_go.merge(ec2go, left_on='GO', right_on=1, how='outer')\n",
    "blast_go = blast_go.rename(columns={'0_x':0,'0_y': 'enzyme'})\n",
    "blast_go = blast_go[[0,'GO', 7, 'enzyme']]    \n",
    "blast_go = blast_go[~blast_go[0].isnull()]\n",
    "blast_go.to_csv(os.path.join(RESULTS, DATASET, 'blastp_go.tsv.gz'), header=None, sep='\\t', index=None, compression='gzip') \n",
    "display(blast_go.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df4 = blast_go[[0, 'GO']]\n",
    "df4 = df4.groupby(0).count().reset_index().merge(trans_len, on=0).rename(columns={0:'Transcript', 1:'Length'}).sort_values('Length')\n",
    "\n",
    "df6 = df4[df4['Length'] <= 5000]\n",
    "print('{}/{}'.format(len(df6), len(df4)))\n",
    "\n",
    "fig = plt.figure(figsize=(16,10), constrained_layout=True)\n",
    "\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[0, :])\n",
    "\n",
    "ax0.bar(df6.Length, height=df6.GO)\n",
    "ax0.set_title('Barplot transcript length vs no. Go Terms');\n",
    "ax0.set_ylabel(\"No of GO Terms\")\n",
    "ax0.set_xlabel(\"Transcript Length\")\n",
    "\n",
    "ax1 = fig.add_subplot(gs[1, 0])\n",
    "\n",
    "n, bins, patches = ax1.hist(df6.GO, 100, facecolor='blue', alpha=0.5)\n",
    "ax1.set_xlabel('No of GO Terms')\n",
    "ax1.set_ylabel('No of Transcripts')\n",
    "ax1.set_title('Histogram of GO Terms')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "n, bins, patches = ax2.hist(df6.Length, 100, facecolor='blue', alpha=0.5)\n",
    "ax2.set_xlabel('Transcript Length')\n",
    "ax2.set_ylabel('')\n",
    "ax2.set_title('Histogram of Transcript Length')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}