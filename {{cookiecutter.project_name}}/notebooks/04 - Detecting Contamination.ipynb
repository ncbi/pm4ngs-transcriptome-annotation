{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb43e75",
   "metadata": {},
   "source": [
    "### Detecting contamination in raw data\n",
    "\n",
    "We use Blastn to detect reads with Blast hits out of the *Viridiplantae* kingdom (TAXID: 33090).\n",
    "This contamination should be removed before assembling the transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a88c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../config/init.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16408f9",
   "metadata": {},
   "source": [
    "### Loading data from {{ cookiecutter.dataset_name }}/sample_table.csv accession list\n",
    "\n",
    "The file `{{ cookiecutter.dataset_name }}/sample_table.cs` should contains a single column with all SRA IDs to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e725d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(DATA, DATASET)\n",
    "result_dir = working_dir(os.path.join(RESULTS, DATASET, 'contamination_cleanup'))\n",
    "\n",
    "sra_df = pandas.read_csv(os.path.join(DATA, DATASET, 'sample_table.csv'), header=None)\n",
    "sra_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090adea8",
   "metadata": {},
   "source": [
    "## Testing gcloud configuration\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### [Cloud SDK](https://cloud.google.com/sdk)\n",
    "\n",
    "\n",
    "Run *gcloud init* to initialize the gcloud environment and follow its instructions:\n",
    "\n",
    " `$ gcloud init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff93a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "account = !gcloud config get-value account\n",
    "account = ''.join(account)\n",
    "project = !gcloud config get-value project\n",
    "project = ''.join(project)\n",
    "if account != '(unset)' and project != '(unset)':\n",
    "    print('Using account: {} with project: {}'.format(account, project))\n",
    "else:\n",
    "    print('Please, configure Cloud SDK before running this notebook')\n",
    "    print('Open a Terminal and run: gcloud init')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb3d888",
   "metadata": {},
   "source": [
    "### Defining variables\n",
    "\n",
    "Edit GCP zone and region variable accordingly to your geographical location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb2ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-east4'\n",
    "ZONE = 'us-east4-b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ae89ff",
   "metadata": {},
   "source": [
    "### Retrieve GCP storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_list = !gsutil ls\n",
    "bucket = None\n",
    "\n",
    "prefix = 'gs://{}-vector-'.format(DATASET.lower())\n",
    "for l in bucket_list:\n",
    "    if prefix in l:\n",
    "        bucket = l.replace('gs://{}-vector-'.format(DATASET.lower()),'').replace('/','')\n",
    "        break\n",
    "\n",
    "vector_bucket  = '{}-vector-{}'.format(DATASET.lower(),bucket)\n",
    "print('vector bucket: {0}'.format(vector_bucket))\n",
    "\n",
    "for s in sra_df[0].unique():\n",
    "    out_bucket = '{}-{}'.format(s.lower(),bucket)\n",
    "        \n",
    "    bucket_list = !gsutil ls gs://{out_bucket}\n",
    "    if ''.join(bucket_list).startswith('BucketNotFoundException'):\n",
    "        !gsutil mb gs://{out_bucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run blastn to detect contaminated reads\n",
    "\n",
    "#### Splitting the FASTA files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "operations = {}\n",
    "PIPELINE = os.path.join(BIN, 'gcp', 'pipeline-split-fasta.json')\n",
    "\n",
    "QUERYSIZE = 100000\n",
    "\n",
    "os.chdir(result_dir)\n",
    "op_dir = os.path.join(result_dir, 'gcp')\n",
    "if not os.path.exists(op_dir):\n",
    "    os.mkdir(op_dir)\n",
    "os.chdir(op_dir)\n",
    "\n",
    "if os.path.exists('operations-split-fasta.tsv'):\n",
    "    operations['logs'] = {}\n",
    "    operations['operations'] = pandas.read_csv('operations-split-fasta.tsv', sep='\\t')\n",
    "else:\n",
    "    d = []\n",
    "    for s in sra_df[0].unique():\n",
    "        out_bucket = '{}-{}'.format(s.lower(),bucket)\n",
    "        a = !gcloud beta lifesciences pipelines run --pipeline-file={PIPELINE} --env-vars=OUTBUCKET={out_bucket},INBUCKET={vector_bucket},SAMPLE={s},SEQNUMBER={QUERYSIZE}\n",
    "        if len(a) == 1 and a[0].startswith('Running'):\n",
    "            a = a[0].replace('].','').split('/')[5]\n",
    "            d.append([s, a, 'running'])\n",
    "        else:\n",
    "            d.append([s, None, a])\n",
    "    operations['logs'] = {}\n",
    "    operations['operations'] = pandas.DataFrame(d, columns=['sample', 'id', 'status'])\n",
    "    operations['operations'].to_csv('operations-split-fasta.tsv', sep='\\t', index=None)\n",
    "\n",
    "display(operations['operations'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(op_dir)\n",
    "\n",
    "df = operations['operations'].dropna()\n",
    "data = []\n",
    "for i, r in df.iterrows():\n",
    "    id = r['id']\n",
    "    if os.path.exists('{}.json.gz'.format(r['sample'])):\n",
    "        with gzip.GzipFile('{}.json.gz'.format(r['sample']), 'r') as fin:\n",
    "            operations['logs'][r['sample']] = json.loads(fin.read().decode('utf-8'))\n",
    "    else:\n",
    "        if r['sample'] not in operations['logs']:\n",
    "            a = !gcloud beta lifesciences operations describe --format=json {id}\n",
    "            l = json.loads(''.join(a))\n",
    "            if 'done' in l and 'error' not in l:\n",
    "                operations['logs'][r['sample']] = l\n",
    "                with gzip.GzipFile('{}.json.gz'.format(r['sample']), 'w') as fout:   # 4. gzip\n",
    "                    fout.write(json.dumps(l, indent=2).encode('utf-8'))\n",
    "    if r['sample'] in operations['logs']:\n",
    "        ts = get_gpc_starttimestamp(operations['logs'][r['sample']])\n",
    "        ts = datetime.strptime(ts.split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        te = datetime.strptime(operations['logs'][r['sample']]['metadata']['endTime'].split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        elapsed = te - ts\n",
    "        data.append([r['sample'], elapsed])\n",
    "operations['gcp'] = pandas.DataFrame(data, columns=['Sample', 'Time'])\n",
    "operations['gcp']['Time'] = operations['gcp']['Time']/pandas.Timedelta('1 minute')\n",
    "display(operations['gcp'])\n",
    "\n",
    "MACHINE_PRICE = 0.02 # n1-standard-2 preemptible\n",
    "print('Computing cost: $ {:.2f}'.format(operations['gcp']['Time'].sum() * MACHINE_PRICE/60))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Running Blast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "operations = {'operations': pandas.DataFrame(columns=['sample', 'id', 'status'])}\n",
    "PIPELINE = os.path.join(BIN, 'gcp', 'pipeline-blastn.json')\n",
    "\n",
    "os.chdir(result_dir)\n",
    "op_dir = os.path.join(result_dir, 'gcp')\n",
    "if not os.path.exists(op_dir):\n",
    "    os.mkdir(op_dir)\n",
    "os.chdir(op_dir)\n",
    "\n",
    "if os.path.exists('operations-blastn.tsv'):\n",
    "    operations['logs'] = {}\n",
    "    operations['operations'] = pandas.read_csv('operations-blastn.tsv', sep='\\t')\n",
    "\n",
    "    count = 0\n",
    "for s in sra_df[0].unique():\n",
    "    out_bucket = '{}-{}'.format(s.lower(),bucket)\n",
    "    files = !gsutil ls gs://{out_bucket}/*.fsa.gz\n",
    "    for f in files:\n",
    "        file_name = f.replace('gs://{}/'.format(out_bucket),'').replace('.fsa.gz','')\n",
    "        if operations['operations'][operations['operations']['sample'] == '{}_{}'.format(s,file_name)].empty:\n",
    "            count += 1\n",
    "            a = !gcloud beta lifesciences pipelines run --pipeline-file={PIPELINE} --env-vars=OUTBUCKET={out_bucket},INBUCKET={out_bucket},FILENAME={file_name}\n",
    "            if len(a) == 1 and a[0].startswith('Running'):\n",
    "                a = a[0].replace('].','').split('/')[5]\n",
    "                operations['operations'].append({\n",
    "                    'sample':'{}_{}'.format(s,file_name),\n",
    "                    'id':a,\n",
    "                    'status':'running'}, ignore_index=True)\n",
    "            else:\n",
    "                operations['operations'].append({\n",
    "                    'sample':'{}_{}'.format(s,file_name),\n",
    "                    'id': None,\n",
    "                    'status':a}, ignore_index=True)\n",
    "operations['logs'] = {}\n",
    "operations['operations'].to_csv('operations-blastn.tsv', sep='\\t', index=None)\n",
    "print('Jobs submitted: {}'.format(count))\n",
    "display(operations['operations'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking completed blast jobs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for s in sra_df[0].unique():\n",
    "    out_bucket = '{}-{}'.format(s.lower(),bucket)\n",
    "    fasta_files = !gsutil ls gs://{out_bucket}/*.fsa.gz\n",
    "    blast_files = !gsutil ls gs://{out_bucket}/*.tsv\n",
    "    total += len(fasta_files)\n",
    "    count += len(blast_files)\n",
    "    print('{} {}/{} total {}/{}'.format(s, len(blast_files),len(fasta_files), count,total))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.chdir(op_dir)\n",
    "\n",
    "df = operations['operations'].dropna()\n",
    "operations['logs'] = {}\n",
    "data = []\n",
    "errors = 0\n",
    "errors_ids = []\n",
    "total = len(df)\n",
    "count = 0\n",
    "completed = 0\n",
    "for i, r in df.iterrows():\n",
    "    count += 1\n",
    "    print('{}/{} {:.2f}% Completed: {} Error: {}\\r'.format(count,\n",
    "                                                           total,\n",
    "                                                           count*100/total,\n",
    "                                                           len(operations['logs']),\n",
    "                                                           errors), end='')\n",
    "    id = r['id']\n",
    "    if os.path.exists('{}.json.gz'.format(r['sample'])):\n",
    "        with gzip.GzipFile('{}.json.gz'.format(r['sample']), 'r') as fin:\n",
    "            operations['logs'][r['sample']] = json.loads(fin.read().decode('utf-8'))\n",
    "    else:\n",
    "        a = !gcloud beta lifesciences operations describe --format=json {id}\n",
    "        l = json.loads(''.join(a))\n",
    "        if 'done' in l and l['done'] and 'error' not in l:\n",
    "            operations['logs'][r['sample']] = l\n",
    "            with gzip.GzipFile('{}.json.gz'.format(r['sample']), 'w') as fout:   # 4. gzip\n",
    "                fout.write(json.dumps(l, indent=2).encode('utf-8'))\n",
    "        elif 'error' in l and 'code' in l['error']:\n",
    "            errors += 1\n",
    "            errors_ids.append(r['sample'])\n",
    "    if r['sample'] in operations['logs']:\n",
    "        ts = get_gpc_starttimestamp(operations['logs'][r['sample']])\n",
    "        ts = datetime.strptime(ts.split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        te = datetime.strptime(operations['logs'][r['sample']]['metadata']['endTime'].split('.')[0],\n",
    "                               \"%Y-%m-%dT%H:%M:%S\")\n",
    "        elapsed = te - ts\n",
    "        data.append([r['sample'], elapsed])\n",
    "print()\n",
    "operations['gcp'] = pandas.DataFrame(data, columns=['Sample', 'Time'])\n",
    "operations['gcp']['Time'] = operations['gcp']['Time']/pandas.Timedelta('1 minute')\n",
    "display(operations['gcp'])\n",
    "\n",
    "df = df[~df['sample'].isin(errors_ids)]\n",
    "df.to_csv('operations-blastn.tsv', sep='\\t', index=None)\n",
    "\n",
    "MACHINE_PRICE = 0.24 # n1-standard-16 preemptible with two local SSDs\n",
    "print('Computing cost: $ {:.2f}'.format(operations['gcp']['Time'].sum() * MACHINE_PRICE/60))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "a329953d",
   "metadata": {},
   "source": [
    "### Creating taxonomy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXDUMP_FILE = 'https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz'\n",
    "TAX_DIR = os.path.join(DATA, 'taxonomy')\n",
    "TAX_PICKLE = os.path.join(TAX_DIR, 'taxonomy_networkx.pickle')\n",
    "if not os.path.exists(TAX_DIR):\n",
    "    os.mkdir(TAX_DIR)\n",
    "    \n",
    "os.chdir(TAX_DIR)\n",
    "if not os.path.exists(TAX_PICKLE):\n",
    "    !wget {TAXDUMP_FILE}\n",
    "    !tar xzf taxdump.tar.gz\n",
    "    node_file = 'nodes.dmp'\n",
    "    name_file = 'names.dmp'\n",
    "    tax_id = parse_tax_name_file(name_file)    \n",
    "    print('Taxonomies: {}'.format(len(tax_id)))\n",
    "    tax = nx.DiGraph()\n",
    "    entries = parse_nodes_file(node_file, tax_id)\n",
    "    nodes, edges = zip(*entries)\n",
    "    print('{} nodes created'.format(len(nodes)))\n",
    "    tax.add_nodes_from(nodes)\n",
    "    for e in edges:\n",
    "        if e: \n",
    "            tax.add_edge(*e)\n",
    "    print('Printing pickle file')\n",
    "    pickle.dump(tax, open(TAX_PICKLE, \"wb\"))    \n",
    "    !ls -1 | grep -v `basename {TAX_PICKLE}` | xargs rm -v\n",
    "    !gsutil -m cp taxonomy_networkx.pickle gs://{vector_bucket}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb3e56d",
   "metadata": {},
   "source": [
    "## Removing contaminated reads\n",
    "\n",
    "Set the `TAXID` variable with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = {}\n",
    "PIPELINE = os.path.join(BIN, 'gcp', 'pipeline-contamination-cleanup.json')\n",
    "TAXID = 33090\n",
    "\n",
    "os.chdir(result_dir)\n",
    "op_dir = os.path.join(result_dir, 'gcp')\n",
    "if not os.path.exists(op_dir):\n",
    "    os.mkdir(op_dir) \n",
    "os.chdir(op_dir)\n",
    "       \n",
    "if os.path.exists('operations-contamination-cleanup.tsv'):\n",
    "    operations['logs'] = {}\n",
    "    operations['operations'] = pandas.read_csv('operations-contamination-cleanup.tsv', sep='\\t')\n",
    "else:\n",
    "    d = []\n",
    "    for f in sra_df[0].unique():\n",
    "        blast_bucket = '{}-{}'.format(f.lower(),bucket)\n",
    "        a = !gcloud beta lifesciences pipelines run --pipeline-file={PIPELINE} --env-vars=TAXID={TAXID},BLAST_BUCKET={blast_bucket},OUTBUCKET={vector_bucket},INBUCKET={vector_bucket},SAMPLE={f}\n",
    "        if len(a) == 1 and a[0].startswith('Running'):\n",
    "            a = a[0].replace('].','').split('/')[5]\n",
    "            d.append([f, a, 'running'])\n",
    "        else:\n",
    "            d.append([f, None, a])\n",
    "    operations['logs'] = {}\n",
    "    operations['operations'] = pandas.DataFrame(d, columns=['sample', 'id', 'status'])\n",
    "    operations['operations'].to_csv('operations-contamination-cleanup.tsv', sep='\\t', index=None)\n",
    "\n",
    "display(operations['operations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(op_dir)\n",
    "\n",
    "df = operations['operations'].dropna()\n",
    "data = []\n",
    "for i, r in df.iterrows():\n",
    "    id = r['id']\n",
    "    if os.path.exists('{}_cleanup.json.gz'.format(r['sample'])):\n",
    "        with gzip.GzipFile('{}_cleanup.json.gz'.format(r['sample']), 'r') as fin:\n",
    "            operations['logs'][r['sample']] = json.loads(fin.read().decode('utf-8'))\n",
    "    else:\n",
    "        a = !gcloud beta lifesciences operations describe --format=json {id}\n",
    "        l = json.loads(''.join(a))\n",
    "        if 'done' in l and 'error' not in l:\n",
    "            operations['logs'][r['sample']] = l\n",
    "            with gzip.GzipFile('{}_cleanup.json.gz'.format(r['sample']), 'w') as fout:   # 4. gzip\n",
    "                fout.write(json.dumps(l, indent=2).encode('utf-8'))\n",
    "    if r['sample'] in operations['logs']:\n",
    "        ts = get_gpc_starttimestamp(operations['logs'][r['sample']])\n",
    "        ts = datetime.strptime(ts.split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        te = datetime.strptime(operations['logs'][r['sample']]['metadata']['endTime'].split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "        elapsed = te - ts\n",
    "        data.append([r['sample'], elapsed])\n",
    "operations['gcp'] = pandas.DataFrame(data, columns=['Sample', 'Time'])\n",
    "operations['gcp']['Time'] = operations['gcp']['Time']/pandas.Timedelta('1 minute')\n",
    "display(operations['gcp'])\n",
    "\n",
    "MACHINE_PRICE = 0.02 # n1-standard-2 preemptible\n",
    "print('Computing cost: $ {:.2f}'.format(operations['gcp']['Time'].sum() * MACHINE_PRICE/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b77a41",
   "metadata": {},
   "source": [
    "## Downloading FastQC results from GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2827847",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(result_dir)\n",
    "!gsutil -m -o 'GSUtil:parallel_composite_upload_threshold=150M' -o 'GSUtil:parallel_process_count=4' -o 'GSUtil:parallel_thread_count=4' rsync -x '.*\\.fastq\\.*' gs://{vector_bucket}/ ./ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ff22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_msg = '#### FastQC report\\n'\n",
    "display(Markdown(str_msg))\n",
    "os.chdir(NOTEBOOKS)\n",
    "\n",
    "base_url = 'https://storage.cloud.google.com/{}/'.format(outbucket_name)\n",
    "\n",
    "str_msg = '| Sample | FastQC<br>Report | No of Reads<br>in fastq | Seq<br> Len | %GC '\n",
    "str_msg += '| Poor<br>Quality | Fail<br>Tests |\\n'\n",
    "str_msg += '| --- | --- |--- | --- | --- | --- | --- |\\n'\n",
    "for sample in sra_df[0].unique():\n",
    "{% if cookiecutter.sequencing_technology == 'paired-end' %}\n",
    "    for r in range(1,3):\n",
    "        s = '{}_clean_noCont_{}'.format(sample, r)\n",
    "        str_msg += '| <a href=\"{0}{1}.fastq.gz\" target=\"_blank\">{1}</a>'.format(base_url, s)\n",
    "        str_msg += '| '\n",
    "        str_msg += find_file_print_link_size(result_dir, s, '.html', 'MB', ' --- ')\n",
    "        str_msg += ' |'\n",
    "        f = os.path.relpath(os.path.join(result_dir, s + '_fastqc.zip'))\n",
    "        if os.path.exists(f) and os.path.getsize(f) != 0:\n",
    "            tests, tot_seq, poor_quality, seq_len, gc_content = parse_fastqc_zip(f)\n",
    "            str_msg += \"{:,}\".format(tot_seq) + '|'\n",
    "            str_msg += seq_len + '|'\n",
    "            str_msg += gc_content + '|'\n",
    "            str_msg += str(poor_quality) + '|'\n",
    "            fail_tests = ''\n",
    "            for t in tests:\n",
    "                if tests[t] == 'FAIL':\n",
    "                    if fail_tests:\n",
    "                        fail_tests += '<br>'\n",
    "                    fail_tests += t\n",
    "            str_msg += fail_tests + '|\\n'\n",
    "\n",
    "        else:\n",
    "            str_msg += ' --- | --- | --- | --- | --- |\\n'\n",
    "{% else %}\n",
    "    str_msg += '| <a href=\"{0}{1}.fastq.gz\" target=\"_blank\">{1}</a>'.format(base_url, sample)\n",
    "    str_msg += '| '\n",
    "    str_msg += find_file_print_link_size(result_dir, sample, '.html', 'MB', ' --- ')\n",
    "    str_msg += ' |'\n",
    "    f = os.path.relpath(os.path.join(result_dir, s + '_clean_noCont_fastqc.zip'))\n",
    "    if os.path.exists(f) and os.path.getsize(f) != 0:\n",
    "        tests, tot_seq, poor_quality, seq_len, gc_content = parse_fastqc_zip(f)\n",
    "        str_msg += \"{:,}\".format(tot_seq) + '|'\n",
    "        str_msg += seq_len + '|'\n",
    "        str_msg += gc_content + '|'\n",
    "        str_msg += str(poor_quality) + '|'\n",
    "        fail_tests = ''\n",
    "        for t in tests:\n",
    "            if tests[t] == 'FAIL':\n",
    "                if fail_tests:\n",
    "                    fail_tests += '<br>'\n",
    "                fail_tests += t\n",
    "        str_msg += fail_tests + '|\\n'\n",
    "\n",
    "    else:\n",
    "        str_msg += ' --- | --- | --- | --- | --- |\\n'\n",
    "{% endif %}\n",
    "display(Markdown(str_msg))\n",
    "del str_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14605e",
   "metadata": {},
   "source": [
    "### Delete temporal buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sra_df[0].unique():\n",
    "    out_bucket = '{}-{}'.format(s.lower(),bucket)\n",
    "    !gsutil -m rm -r gs://{out_bucket}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7f871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}