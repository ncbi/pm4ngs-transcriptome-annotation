{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TSA Submission Template\n",
    "\n",
    "https://submit.ncbi.nlm.nih.gov/genbank/template/submission/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../config/init.py\n",
    "import itertools\n",
    "from goenrichment.go import parse_go_obo\n",
    "from Bio.ExPASy import Enzyme\n",
    "THREADS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download latest copy of tbl2asn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(BIN,'tbl2asn')):\n",
    "    !wget -O {BIN}/tbl2asn.gz https://ftp.ncbi.nih.gov/toolbox/ncbi_tools/converters/by_program/tbl2asn/linux.tbl2asn.gz\n",
    "    !gunzip -v {BIN}/tbl2asn.gz\n",
    "    !chmod a+x {BIN}/tbl2asn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download enzyme database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(DATA,'enzyme.dat')):\n",
    "    !wget -O {DATA}/enzyme.dat ftp://ftp.expasy.org/databases/enzyme/enzyme.dat\n",
    "enzyme = {}\n",
    "with open(os.path.join(DATA,'enzyme.dat'), 'r') as e_fin:\n",
    "    for r in Enzyme.parse(e_fin):\n",
    "        n = r['DE'][:-1]\n",
    "        if n[0].isupper() and n[1] != '-' and not n[1].isupper():\n",
    "            n = n[0].lower() + n[1:]\n",
    "        \n",
    "        enzyme[r['ID']] = n\n",
    "print('{} enzymes loaded'.format(len(enzyme)))\n",
    "\n",
    "def validate_ec_number(enzyme, ec):\n",
    "    if enzyme[ec].startswith('transferred entry'):\n",
    "        for j in re.sub(' +', ' ', enzyme[ec].replace('transferred entry: ', '').replace('and', '').strip()).split(' '):\n",
    "            try:\n",
    "                return j\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        return ec\n",
    "    raise Exception()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading GO graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go = nx.DiGraph()\n",
    "entries = parse_go_obo(os.path.join(DATA, 'go-basic.obo'))\n",
    "nodes, edges = zip(*entries)\n",
    "go.add_nodes_from(nodes)\n",
    "go.add_edges_from(itertools.chain.from_iterable(edges))\n",
    "go.graph['roots'] = {data['name']: n for n, data in go._node.items() \\\n",
    "                     if 'name' in data and data['name'] == data['namespace']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading sample list from GCP operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pandas.read_csv(os.path.join(RESULTS, DATASET, 'annotation', 'gcp', 'operations.tsv'), sep='\\t')\n",
    "samples = samples['sample']\n",
    "bar_length = len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = os.path.join(RESULTS, DATASET, 'submission')\n",
    "if not os.path.exists(result_dir):\n",
    "    os.mkdir(result_dir)\n",
    "os.chdir(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading BlastP, blast2GO, blast2CDD amd Enzyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blastp = pandas.read_csv(os.path.join(RESULTS, DATASET, 'annotation', 'blastp.tsv.gz'), header=None, sep='\\t')\n",
    "blastp = blastp.sort_values(by=[0,4])\n",
    "blastp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_go = pandas.read_csv(os.path.join(RESULTS, DATASET, 'blastp_go.tsv.gz'), header=None, sep='\\t')\n",
    "blast_go = blast_go.sort_values(by=[0,2])\n",
    "def format_go(go, g):\n",
    "    try: \n",
    "        g = go.nodes()[g]\n",
    "        return ('go_{}'.format(g['namespace'].split('_')[1]), \n",
    "                '{}|{}||'.format(g['name'],g['id'][3:]))\n",
    "    except:\n",
    "        return g\n",
    "blast_go[4] = blast_go.apply(lambda x: format_go(go, x[1]), axis=1)\n",
    "blast_go.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdd = pandas.read_csv(os.path.join(RESULTS, DATASET, 'cdd.tsv.gz'), header=None, sep='\\t')\n",
    "cdd = cdd.replace(np.nan, '', regex=True)\n",
    "cdd.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Contamination.txt file returned after a submission to clean up the submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$RESULTS\" \"$DATASET\"\n",
    "\n",
    "cd $1/$2/submission\n",
    "if [ -e \"Contamination.txt\" ]\n",
    "then\n",
    "    cat Contamination.txt | grep \"^TRINITY_\" | awk '{print $1}' > trans_to_remove.tsv\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(result_dir)\n",
    "id_err = pandas.DataFrame(columns=[0])\n",
    "if os.path.exists('trans_to_remove.tsv'):\n",
    "    id_err = pandas.read_csv('trans_to_remove.tsv', header=None, sep=' ')\n",
    "display(id_err.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TBL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def worker(s):\n",
    "    \"\"\"worker function\"\"\" \n",
    "    rfromto = re.compile(':([0-9]+)-([0-9]+)')\n",
    "    dna = os.path.join(RESULTS, DATASET, 'annotation', 'blasts', s, s + '_nocont.fsa.gz')\n",
    "    prot = os.path.join(RESULTS, DATASET, 'annotation', 'blasts', s, s + '_nocont_transdecoder.fsa.gz') \n",
    "    fasta = {}\n",
    "    with gzip.open(dna, \"rt\") as dna_handle:\n",
    "        for dna_record in SeqIO.parse(dna_handle, \"fasta\"):\n",
    "            if len(dna_record.seq) >= 200 and dna_record.id.replace('|', '_') not in id_err[0].unique():\n",
    "                fasta[dna_record.id] = {'f': dna_record, 'r': False}\n",
    "    df_blast_p = blastp[blastp[0].isin(fasta)]\n",
    "    df_blast_go = blast_go[blast_go[0].isin(fasta)]\n",
    "    df_cdd = cdd[cdd[0].isin(fasta)]\n",
    "    prots = {} \n",
    "    with gzip.open(prot, \"rt\") as prot_handle:          \n",
    "        for prot_record in SeqIO.parse(prot_handle, \"fasta\"):\n",
    "            f = prot_record.id.split('.')\n",
    "            if f[0] in fasta:\n",
    "                p = prots.setdefault(f[0], {})   \n",
    "                d = prot_record.description.split(' ')\n",
    "                m = rfromto.search(d[4])\n",
    "                pFrom = int(m.group(1))\n",
    "                pTo = int(m.group(2))\n",
    "                d = d[1][5:]\n",
    "\n",
    "                df = df_blast_p[(df_blast_p[0] == f[0])&(df_blast_p[7] == f[1])]\n",
    "                blast_len = len(df)\n",
    "                g = None\n",
    "                e = None\n",
    "                c = None\n",
    "                cc = None\n",
    "                g_in = df[(~df[2].str.contains('_')) & (df[2].str.contains('.'))].sort_values(by=4, ascending=False).head(15)\n",
    "                r_in = df[(df[2].str.contains('_')) & (df[2].str.contains('.'))].sort_values(by=4, ascending=False).head(15)\n",
    "                notes = df[~df[2].str.contains('.')].sort_values(by=4, ascending=False).head(10)\n",
    "                if not df.empty:\n",
    "                    df = df[df[4] == df[4].max()] \n",
    "                    df = df_blast_go[(df_blast_go[0] == f[0])&(df_blast_go[2] == f[1])]\n",
    "                    if not df.empty:\n",
    "                        g = df[[0,4]].drop_duplicates()[4].tolist()\n",
    "                        e = df[[0,3]].drop_duplicates()[3].dropna().tolist()\n",
    "                    df = df_cdd[(df_cdd[0] == f[0])&(df_cdd[4] == f[1])]\n",
    "                    df = pandas.concat([df, df_cdd[(df_cdd[0] == f[0])&(df_cdd[4] == '')]])\n",
    "                    if not df.empty:\n",
    "                        c = df[2].unique().tolist()\n",
    "                        cc = df.sort_values(by=3, ascending=False)[[0,1]].drop_duplicates().head(15)\n",
    "                if not p:\n",
    "                    p['l'] = len(prot_record.seq)\n",
    "                    p['f'] = pFrom\n",
    "                    p['t'] = pTo\n",
    "                    p['d'] = d\n",
    "                    p['b'] = blast_len\n",
    "                    p['g_in'] = g_in[2].to_list()\n",
    "                    p['r_in'] = r_in[2].to_list()\n",
    "                    p['notes'] = notes[2].to_list()\n",
    "                    p['g'] = []\n",
    "                    p['e'] = []\n",
    "                    p['c'] = []\n",
    "                    p['cc'] = []\n",
    "                    if g:\n",
    "                        p['g'] = g\n",
    "                    if e:\n",
    "                        p['e'] = e\n",
    "                    if c:\n",
    "                        p['c'] = c\n",
    "                        p['cc'] = cc[1].to_list()\n",
    "                else:        \n",
    "                    replace = True if p['d'] != 'complete' and d == 'complete' else False\n",
    "                    if not replace and p['d'] != 'complete':\n",
    "                        replace = True if p['b'] < blast_len else False\n",
    "                    if replace: \n",
    "                        prots[f[0]]['l'] = len(prot_record.seq)\n",
    "                        prots[f[0]]['f'] = pFrom\n",
    "                        prots[f[0]]['t'] = pTo\n",
    "                        prots[f[0]]['d'] = d\n",
    "                        prots[f[0]]['b'] = blast_len\n",
    "                        prots[f[0]]['g_in'] = g_in[2].to_list()\n",
    "                        prots[f[0]]['r_in'] = r_in[2].to_list()\n",
    "                        prots[f[0]]['notes'] = notes[2].to_list()\n",
    "                        prots[f[0]]['g'] = []\n",
    "                        prots[f[0]]['e'] = []\n",
    "                        prots[f[0]]['c'] = []\n",
    "                        if g:\n",
    "                            prots[f[0]]['g'] = g\n",
    "                        if e:\n",
    "                            prots[f[0]]['e'] = e\n",
    "                        if c:\n",
    "                            prots[f[0]]['c'] = c\n",
    "                            prots[f[0]]['cc'] = cc[1].to_list()\n",
    "            \n",
    "                        \n",
    "    with open(os.path.join(result_dir, s + '.tbl'), \"w\") as tbl_output:\n",
    "        for k, p in prots.items():\n",
    "            tbl_output.write('>Feature\\t{}\\n'.format(k.replace('|', '_')))\n",
    "            \n",
    "            if p['f'] > p['t']: \n",
    "                seqLen = len(fasta[k]['f'].seq)\n",
    "                p['f'] = seqLen + 1 - p['f']\n",
    "                p['t'] = seqLen + 1 - p['t']\n",
    "                fasta[k]['r'] = True\n",
    "            \n",
    "            if p['d'] =='3prime_partial':\n",
    "                tbl_output.write('{}\\t>{}'.format(p['f'], p['t']))\n",
    "            elif p['d'] =='5prime_partial':\n",
    "                tbl_output.write('<{}\\t{}'.format(p['f'], p['t']))\n",
    "            elif p['d'] =='internal':\n",
    "                tbl_output.write('<{}\\t>{}'.format(p['f'], p['t']))\n",
    "            else:\n",
    "                tbl_output.write('{}\\t{}'.format(p['f'], p['t']))\n",
    "            \n",
    "            tbl_output.write('\\tCDS\\n')\n",
    "            if len(p['e']) == 1:\n",
    "                l = p['e'][0].replace('EC:', '')\n",
    "                if len(l.split('.')) == 4:\n",
    "                    try:\n",
    "                        l = validate_ec_number(enzyme, l)                            \n",
    "                        tbl_output.write('\\t\\t\\tproduct\\t{}\\n'.format(enzyme[l]))\n",
    "                        tbl_output.write('\\t\\t\\tEC_number\\t{}\\n'.format(l))\n",
    "                    except:\n",
    "                        tbl_output.write('\\t\\t\\tproduct\\thypothetical protein\\n')\n",
    "                else:\n",
    "                    tbl_output.write('\\t\\t\\tproduct\\thypothetical protein\\n')\n",
    "            elif len(p['e']) >= 1:\n",
    "                e = []\n",
    "                for g in p['e']:\n",
    "                    l = g.replace('EC:', '')\n",
    "                    if len(l.split('.')) == 4:\n",
    "                        e.append(l)\n",
    "                if len(e) == 1:\n",
    "                    try:\n",
    "                        l = validate_ec_number(enzyme, e[0]) \n",
    "                        tbl_output.write('\\t\\t\\tproduct\\t{}\\n'.format(enzyme[l]))\n",
    "                        tbl_output.write('\\t\\t\\tEC_number\\t{}\\n'.format(l))\n",
    "                    except:\n",
    "                        tbl_output.write('\\t\\t\\tproduct\\thypothetical protein\\n')\n",
    "                else:                    \n",
    "                    tbl_output.write('\\t\\t\\tproduct\\thypothetical protein\\n') \n",
    "            else:\n",
    "                tbl_output.write('\\t\\t\\tproduct\\thypothetical protein\\n')\n",
    "            \n",
    "            start_codon = 1 if p['f'] > 1 and p['f'] <= 3 else p['f']\n",
    "            tbl_output.write('\\t\\t\\tcodon_start\\t{}\\n'.format(start_codon))   \n",
    "            \n",
    "            for i in p['g_in']:\n",
    "                tbl_output.write('\\t\\t\\tinference\\talignment:blastp:2.9.0:INSD:{0}\\n'.format(i))\n",
    "            for i in p['r_in']:\n",
    "                tbl_output.write('\\t\\t\\tinference\\talignment:blastp:2.9.0:RefSeq:{0}\\n'.format(i))            \n",
    "\n",
    "            for i in p['cc']:\n",
    "                tbl_output.write('\\t\\t\\tinference\\talignment:rpsblastp:2.9.0:CDD:{0}\\n'.format(i))\n",
    "                \n",
    "            for g in sorted(p['g'], key=lambda x: x[0], reverse=True):\n",
    "                if isinstance(g, tuple):\n",
    "                    tbl_output.write('\\t\\t\\t')\n",
    "                    tbl_output.write('\\t'.join(g))\n",
    "                    tbl_output.write('\\n')\n",
    "            \n",
    "            for g in sorted(p['g'], key=lambda x: x[0], reverse=True):\n",
    "                if isinstance(g, tuple):\n",
    "                    tbl_output.write('\\t\\t\\tdb_xref\\tGO:{}\\n'.format(g[1].split('|')[1]))\n",
    "                else:\n",
    "                    tbl_output.write('\\t\\t\\tdb_xref\\t{}\\n'.format(g))\n",
    "            \n",
    "            for g in p['c']:\n",
    "                tbl_output.write('\\t\\t\\tdb_xref\\tCDD:{}\\n'.format(g)) \n",
    "    \n",
    "    with open(os.path.join(result_dir, s + '.fsa'), \"w\") as fasta_handle:\n",
    "        for k, f in fasta.items():\n",
    "            f['f'].description = ''\n",
    "            if f['r']:\n",
    "                f['f'].seq = f['f'].seq.reverse_complement()\n",
    "            SeqIO.write(f['f'], fasta_handle, \"fasta\")\n",
    "    print('{} with {} CDS'.format(s, len(prots)))\n",
    "                \n",
    "p = Pool(processes=THREADS)\n",
    "data = p.map(worker, samples)\n",
    "p.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing | from the transcript name\n",
    "\n",
    "tbl2asn produces error if the | character is in the transcript name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for a in *.fsa\n",
    "do\n",
    "    sed 's/|/_/' $a > t\n",
    "    mv -v t $a\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run thes commands in a terminal to generate submission files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(result_dir)\n",
    "print('COMMAND #1: cd {}'.format(result_dir))\n",
    "print()\n",
    "print('COMMAND #2: {}/tbl2asn -p . -j \"[organism=Opuntia streptacantha][moltype=transcribed_RNA][tech=TSA][gcode=1]\" -V tvb -a s -t template.sbt  -c x -M t'.format(BIN))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}